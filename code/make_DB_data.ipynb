{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Database Management Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from retrying import retry\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import dill\n",
    "import time\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import unicodedata\n",
    "import csv\n",
    "import sys\n",
    "import get_stack_overflow_data as gso\n",
    "from string import strip,split\n",
    "sys.path.append('../code')\n",
    "sys.path.append('../data')\n",
    "import re\n",
    "\n",
    "#function that adds a delay before running a function, to be used as a decorator.\n",
    "def sleeper(secs):\n",
    "    def realsleeper(func):\n",
    "        def wrapper(*args,**kwargs):\n",
    "            time.sleep(secs)\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapper\n",
    "    return realsleeper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of database management systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 https://db-engines.com/en/ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1 = requests.get('https://db-engines.com/en/ranking')\n",
    "soup1 = BeautifulSoup(r1.text, 'lxml')\n",
    "db_list1 = []\n",
    "for atag in soup1.findAll('a', attrs={'class':None}):\n",
    "    if (type(atag.contents[0]) == bs4.element.NavigableString):\n",
    "        print atag.contents[0]\n",
    "        db_list1.append(atag.contents[0])\n",
    "    else:\n",
    "        db_list1.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CODE FOR CLEANING UP db_list1\n",
    "\n",
    "#Set of extraneous items to remove from the list:\n",
    "items_to_remove1 = set(['Relational DBMS','Document store','Key-value store','Search engine','Wide column store','Multi-model','Graph DBMS','Time Series DBMS','Content store','Navigational DBMS','Object oriented DBMS','Native XML DBMS','Event Store','RDF store'])\n",
    "\n",
    "db_set1 = set(db_list1[45:697]) #a set used in cleaning up db_list1. As of 3-25-18, the index range [48:697] was the correct one to use. This may change if and when the ranking list is updated.\n",
    "db_set1.difference_update(items_to_remove1)\n",
    "\n",
    "#opensource(db_name) returns True if db_name is an open source database engine, false otherwise\n",
    "def opensource(db_name):\n",
    "    url = 'https://db-engines.com/en/system/' + db_name\n",
    "     \n",
    "    url = url.replace('\\\\x','%')\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if re.search('>Open Source',r.text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#remove databases engines that are not open source\n",
    "db_list1 = [db_name for db_name in db_list1 if opensource(db_name)]\n",
    "\n",
    "#encode as utf-8, make lowercase, and remove whitespace\n",
    "db_list1 = [strip(x).encode('utf-8').lower() for x in db_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(db_list1)\n",
    "print sorted(db_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 https://en.wikipedia.org/wiki/List_of_relational_database_management_systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1 = requests.get('https://en.wikipedia.org/wiki/List_of_relational_database_management_systems')\n",
    "soup2 = BeautifulSoup(r1.text, 'lxml')\n",
    "db_list2 = []\n",
    "for litags in soup2.findAll('li', attrs={'class':None}):\n",
    "    try:\n",
    "        for atag in litags.find('a'):\n",
    "            db_list2.append(atag.encode('utf-8').lower())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use split to remove expressions in parentheses, which are notes about the items in the list.\n",
    "#As of 3-25-18, the index range [:96] was the correct one to use. This may change if and when the list is updated.\n",
    "db_list2 = [split(x, r' (')[0] for x in db_list2[:96]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(db_list2)\n",
    "print len(filter(lambda x:(x not in db_list1),db_list2))\n",
    "print filter(lambda x:(x not in db_list1),db_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove items in db_list2 that are duplicates of items in db_list1 by a different name (e.g. 4d vs. 4th dimension)\n",
    "print len(db_list2)\n",
    "db_list2_additional_duplicates = ['4th dimension','adabas d','apache derby','ca datacom','ca idms','empress embedded database','exasolution','filemaker pro','sql azure','openlink virtuoso','openlink virtuoso universal server','postgres plus advanced server','progress software','sap adaptive server enterprise','sql anywhere','unidata','universe']\n",
    "db_list2 = filter(lambda x:(x not in db_list2_additional_duplicates),db_list2)\n",
    "print len(db_list2)\n",
    "\n",
    "#remove items in db_list2 that are not open-source\n",
    "\n",
    "db_list2_not_open_source = ['alpha five',\n",
    "'aster data',\n",
    "'clarion',\n",
    "'grovesite',\n",
    "'helix database',\n",
    "'ibm db2',\n",
    "'ibm lotus approach',\n",
    "'ibm db2 express-c',\n",
    "'intersystems cach\\xc3\\xa9',\n",
    "'microsoft jet database engine',\n",
    "'microsoft sql server express',\n",
    "'microsoft visual foxpro',\n",
    "'omnis studio',\n",
    "'panorama',\n",
    "'pervasive psql',\n",
    "'polyhedra',\n",
    "'rdm server', \n",
    "'sand cdbms', \n",
    "'unisys rdms 2200',\n",
    "'vectorwise']\n",
    "\n",
    "db_list2 = filter(lambda x:(x not in db_list2_not_open_source),db_list2)\n",
    "\n",
    "print len(db_list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_list12 = list(set(db_list1 + db_list2))\n",
    "print(sorted(db_list12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 https://www.quora.com/Which-is-the-best-database-for-big-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#items entered manually from this Quora discussion (only entering items that are open-source)\n",
    "db_list3 = ['hive','hbase','apache phoenix','mongodb','druid','mapd','couchdb','hbase','riak','zookeeper','cassandra','voldemort']\n",
    "\n",
    "print len(db_list3)\n",
    "db_list3_additional_duplicates = ['riak','voldemort']\n",
    "db_list3 = filter(lambda x:(x not in db_list3_additional_duplicates),db_list3)\n",
    "print len(db_list3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine and edit lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_list = list(set(db_list1 + db_list2 + db_list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add oracle to db_list, since it is listed under different names\n",
    "#db_list.append('oracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(db_list)\n",
    "print sorted(db_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('db_list.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(db_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove database engines that are proprietary with no github repository\n",
    "db_list.remove('microsoft access')\n",
    "db_list.remove('microsoft sql server')\n",
    "db_list.remove('oracle')\n",
    "db_list.remove('oracle nosql')\n",
    "db_list.remove('oracle rdb')\n",
    "db_list.remove('clustrix')\n",
    "db_list.remove('enterprisedb')\n",
    "db_list.remove('infobright')\n",
    "db_list.remove('linter')\n",
    "db_list.remove('maxdb')\n",
    "db_list.remove('memsql')\n",
    "db_list.remove('mimer sql')\n",
    "db_list.remove('nexusdb')\n",
    "db_list.remove('nonstop sql')\n",
    "db_list.remove('nuodb')\n",
    "db_list.remove('openbase')\n",
    "db_list.remove('r:base')\n",
    "db_list.remove('rdm embedded')\n",
    "db_list.remove('sap hana')\n",
    "db_list.remove('sap iq')\n",
    "db_list.remove('soliddb')\n",
    "db_list.remove('splice machine')\n",
    "db_list.remove('sqlbase')\n",
    "db_list.remove('sqream db')\n",
    "db_list.remove('tibero')\n",
    "db_list.remove('timesten')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../data/DB_final.txt\", \"w\") as f:\n",
    "    f.writelines(\" \".join(db_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "db_list.sort()\n",
    "print db_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%with open(\"../code/secrets/github-token.nogit\", \"rb\") as f:\n",
    "    token = f.read()\n",
    "\n",
    "headers = {'Authorization': 'token %s' % token}\n",
    "\n",
    "@sleeper(3)#github will temporarily block requests from a user that makes more than 30 requests within a 60 second period. To be safe, use a 3 second pause between requests to limit rate to 20 requests per minute.\n",
    "@retry(wait_exponential_multiplier=3000,wait_exponential_max=12000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 3 second wait period and doubling that period each time.\n",
    "def get_data_from_search_helper(query):\n",
    "    r = requests.get('https://api.github.com/search/repositories?q='+\\\n",
    "                             query, \n",
    "                     headers=headers)\n",
    "    return r\n",
    "\n",
    "def get_data_from_search(query):\n",
    "    \"\"\"Use github search to return stars, forks for top query result\"\"\"\n",
    "    \n",
    "    r = get_data_from_search_helper(query)\n",
    "    #r.raise_for_status()\n",
    "    try:\n",
    "        res = r.json()['items'][0]\n",
    "        return {'toolkit': query, 'full_name': res['full_name'],\n",
    "                'stars': int(res['stargazers_count']), 'forks': int(res['forks_count'])}\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [res for res in (get_data_from_search(q) for q in db_list)\n",
    "        if res is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github = pd.DataFrame(data)[['toolkit', 'full_name', 'forks', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "github.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.width', 160, 'display.max_rows', None,'display.max_columns', None):\n",
    "    print(github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#github search returned wrong repo for many results, so they are manually changed below\n",
    "\n",
    "corrections = [\n",
    "    ('aerospike','aerospike'),\n",
    "    ('boltdb','boltdb/bolt'),\n",
    "    ('couchbase','couchbase'),\n",
    "    ('cratedb','crate/crate'),\n",
    "    ('csql',None),\n",
    "    ('cubicweb',None),\n",
    "    ('database management library',None),\n",
    "    ('dataease',None),\n",
    "    ('db4o',None),\n",
    "    ('dbase',None),\n",
    "    ('djondb',None),\n",
    "    ('ehcache','ehcache'),\n",
    "    ('exist-db','eXist-db/exist'),\n",
    "    ('extremedb',None),\n",
    "    ('frontbase',None),\n",
    "    ('google fusion tables',None),\n",
    "    ('grakn.ai','graknlabs/grakn'),\n",
    "    ('graphite','graphite-project'),\n",
    "    ('griddb','griddb_nosql'),\n",
    "    ('h2','h2database'),\n",
    "    ('hsqldb',None),\n",
    "    ('iboxdb','iboxdb'),\n",
    "    ('ignite','apache/ignite'),\n",
    "    ('impala','apache/impala'),\n",
    "    ('infogrid','infogrid-org'),\n",
    "    ('informix',None),\n",
    "    ('interbase',None),\n",
    "    ('kyoto cabinet','alticelabs/kyoto/kyotocabinet'),\n",
    "    ('kyoto tycoon','alticelabs/kyoto/kyototycoon'),\n",
    "    ('libreoffice base',None),\n",
    "    ('mapd','mapd/mapd-core'),\n",
    "    ('mariadb','MariaDB'),\n",
    "    ('mongodb','mongodb/mongo'),\n",
    "    ('mysql','mysql'),\n",
    "    ('netezza',None),\n",
    "    ('openoffice.org base',None),\n",
    "    ('oracle berkeley db',None),\n",
    "    ('percona server for mongodb','percona/percona-server-mongodb'),\n",
    "    ('percona server for mysql','percona/percona-server'),\n",
    "    ('perst',None),\n",
    "    ('postgresql','postgres/postgres'),\n",
    "    ('prestodb','prestodb/presto'),\n",
    "    ('project voldemort','voldemort/voldemort'),\n",
    "    ('pyrrho',None),\n",
    "    ('redland',None),\n",
    "    ('resin cache',None),\n",
    "    ('riak ts','basho/riak'),\n",
    "    ('sap hana',None),\n",
    "    ('scidb',None),\n",
    "    ('scylladb','scylladb/scylla'),\n",
    "    ('sedna','sedna/sedna'),\n",
    "    ('senseidb','senseidb'),\n",
    "    ('smallsql','kevinvandervlist/SE/tree/master/smallsql'),\n",
    "    ('solr','apache/lucene-solr'),\n",
    "    ('spark sql','apache/spark/tree/master/sql'),\n",
    "    ('sqlite',None),\n",
    "    ('teradata','teradata'),\n",
    "    ('tinkergraph',None),\n",
    "    ('tokyo cabinet',None),\n",
    "    ('tokyo tyrant',None),\n",
    "    ('torodb','torodb/torodb'),\n",
    "    ('txtsql',None),\n",
    "    ('vertica','vertica'),\n",
    "    ('virtuoso','openlink/virtuoso-opensource'),\n",
    "    ('wakandadb','WakandaDB'),\n",
    "    ('Zookeeper','apache/zookeeper')\n",
    "]\n",
    "\n",
    "for toolkit,full_name in corrections:\n",
    "    try:\n",
    "        r = requests.get('https://api.github.com/repos/' + full_name, headers=headers)\n",
    "        res = r.json()\n",
    "        github.loc[github['toolkit'] == toolkit, 'full_name'] = full_name\n",
    "        github.loc[github['toolkit'] == toolkit, 'forks'] = res['forks_count']\n",
    "        github.loc[github['toolkit'] == toolkit, 'stars'] = res['stargazers_count']\n",
    "    except:#to handle None\n",
    "        github.loc[github['toolkit'] == toolkit, 'full_name'] = None\n",
    "        github.loc[github['toolkit'] == toolkit, 'forks'] = 0\n",
    "        github.loc[github['toolkit'] == toolkit, 'stars'] = 0       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "github.sort_values(['stars'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github.to_csv(\"../data/DB_results_github.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stackoverflow stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_list = [toolkit.replace(' ','-').replace(':','-') for toolkit in db_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for getting stack overflow tag counts\n",
    "@sleeper(3)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=60000,wait_exponential_max=240000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 1 minute wait period and doubling that period each time.\n",
    "def tag_counts_builder_helper(list_to_build,list_entry):\n",
    "    list_to_build += gso.get_tag_counts([list_entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build list of tag counts\n",
    "tag_counts = []\n",
    "for x in tag_list:\n",
    "    try:\n",
    "        tag_counts_builder_helper(tag_counts,x)\n",
    "    except:\n",
    "        tag_counts += [{'count':0,'has_synonyms':False,'is_moderator_only':False,'is_required':False,'name':x}]\n",
    "        print 'exception'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tags = pd.DataFrame(tag_counts)[['name', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tags.columns = ['toolkit', 'so_tag_counts']\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sleeper(3)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=60000,wait_exponential_max=240000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 1 minute wait period and doubling that period each time.\n",
    "#function used in building a dict each of whose values is the body count of the corresponding key\n",
    "def body_counts_builder_helper(dict_to_build,key):\n",
    "    dict_to_build[key] = gso.get_body_count([key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build dict of body counts\n",
    "\n",
    "body_counts = {}\n",
    "\n",
    "for x in tag_list:\n",
    "    body_counts_builder_helper(body_counts,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_questions = pd.DataFrame.from_dict(body_counts, orient='index')\n",
    "df_questions.reset_index(inplace=True)\n",
    "df_questions.columns = ['toolkit', 'so_question_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so = df_tags.merge(df_questions, on='toolkit', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so['toolkit'] = so['toolkit'].apply(lambda x: str(x).replace('-',' '))\n",
    "so.sort_values(['so_tag_counts'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so.to_csv(\"../data/DB_results_stackoverflow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google search results stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../code/secrets/google_token.nogit\", \"rb\") as f:\n",
    "    my_api_key = f.read()\n",
    "    \n",
    "with open(\"../code/secrets/cse_token.nogit\", \"rb\") as f:\n",
    "    my_cse_id = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_term_modifier(search_term):\n",
    "    #replace space with +, indicating to search for both words:\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    return search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for returning the number of google search results\n",
    "def google_search_results_count(search_term, api_key, cse_id):\n",
    "    toolkit = search_term\n",
    "    search_term = search_term_modifier(search_term)\n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"database\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=y5&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    return {'toolkit': toolkit, 'search_results': int(res['totalResults'])}\n",
    "\n",
    "#function for returning the rate of growth of google search results\n",
    "def google_quarterly_growth_rate(search_term, api_key, cse_id):\n",
    "    toolkit = search_term\n",
    "    search_term = search_term_modifier(search_term)    \n",
    "    ##get count for last 6 months--- dateRestrict=m6\n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"database\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=m6&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    six_months = int(res['totalResults'])\n",
    "    ##get count for last 3 months--- dateRestrict=m3    \n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"database\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=m3&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    current_quarter = int(res['totalResults'])\n",
    "    \n",
    "    last_quarter = six_months - current_quarter\n",
    "    if (last_quarter == 0):#for handling the divide by 0 case\n",
    "        growth_rate = float('NaN')\n",
    "    else:\n",
    "        growth_rate = (float(current_quarter)-float(last_quarter))/float(last_quarter)\n",
    "    return {'toolkit': toolkit, 'growth_rate': growth_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sleeper(2)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=2000,wait_exponential_max=8000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 2 second period and doubling that period each time.\n",
    "#function used in building a list each of whose values is the google search results count\n",
    "def google_results_builder_helper(list_to_build,query):\n",
    "    res = google_search_results_count(query, my_api_key, my_cse_id)\n",
    "    if res is not None:\n",
    "        list_to_build.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for q in db_list:\n",
    "    google_results_builder_helper(results,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsDF = pd.DataFrame(results)[['toolkit', 'search_results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsDF.sort_values(['search_results'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@sleeper(8)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=8000,wait_exponential_max=32000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 2 second period and doubling that period each time.\n",
    "#function used in building a list each of whose values is the google quarterly growth rate\n",
    "def growth_rate_builder_helper(list_to_build,query):\n",
    "    res = google_quarterly_growth_rate(query, my_api_key, my_cse_id)\n",
    "    if res is not None:\n",
    "        list_to_build.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "growth_rate = []\n",
    "for q in db_list:\n",
    "    growth_rate_builder_helper(growth_rate,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "growthDF =  pd.DataFrame(growth_rate)[['toolkit', 'growth_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "growthDF.sort_values(['growth_rate'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googleDF = growthDF.merge(resultsDF, on='toolkit', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googleDF.to_csv(\"../data/DB_results_google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dltkDF = github.merge(so, on='toolkit', copy = False)\n",
    "dltkDF = dltkDF.merge(googleDF, on='toolkit', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dltkDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dltkDF.to_csv(\"../output/DB_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
