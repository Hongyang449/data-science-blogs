{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Deep Learning and Artificial Intelligence Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from retrying import retry\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import dill\n",
    "import time\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import csv\n",
    "import sys\n",
    "import get_stack_overflow_data as gso\n",
    "sys.path.append('../code')\n",
    "sys.path.append('../data')\n",
    "\n",
    "#function that adds a delay before running a function, to be used as a decorator.\n",
    "def sleeper(secs):\n",
    "    def realsleeper(func):\n",
    "        def wrapper(*args,**kwargs):\n",
    "            time.sleep(secs)\n",
    "            return func(*args,**kwargs)\n",
    "        return wrapper\n",
    "    return realsleeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 https://svds.com/understanding-ai-toolkits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1 = requests.get('https://svds.com/understanding-ai-toolkits/')\n",
    "soup1 = BeautifulSoup(r1.text, 'lxml')\n",
    "toolkit_list1 = []\n",
    "for litags in soup1.findAll('li', attrs={'class':None}):\n",
    "    for atag in litags.find('a'):\n",
    "        toolkit_list1.append(atag.encode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toolkit_list1 = toolkit_list1[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkit_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2 = requests.get('https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software')\n",
    "soup2 = BeautifulSoup(r2.text, 'lxml')\n",
    "toolkit_list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr in soup2.findAll('tr'):\n",
    "    if tr.find('td'):\n",
    "        toolkit_list2.append(tr.find('td').text.encode('utf-8').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkit_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 https://www.packtpub.com/books/content/top-10-deep-learning-frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#by hand, request 403 Forbidden \n",
    "toolkit_list3 = ['tensorflow','theano','keras','caffe','torch','deeplearning4j','mxnet','microsoft cognitive toolkit','lasagne','bigdl'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkit_list3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 https://twitter.com/fchollet/status/882995652233371648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#by hand from jpgs\n",
    "toolkit_list4 = ['tensorflow','keras','mxnet','caffe2','pytorch','caffe','paddle','cntk',\n",
    "                'deeplearning4j','tflearn','dlib','theano','chainer','digits','dynet', 'sonnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkit_list4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 https://svds.com/wp-content/uploads/2017/02/Deep_learning_ratings_final-1024x563.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by hand from png\n",
    "toolkit_list5 = ['theano', 'tensorflow', 'torch', 'caffe', 'mxnet', 'neon', 'cntk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkit_list5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 https://www.quora.com/Which-are-the-artificial-intelligence-libraries-in-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by hand\n",
    "toolkit_list6 = ['tensorflow','scikit-learn','theano','nupic','pybrain','pyevolve','pattern','caffe','shogun','mlpy','scikit-image','opencv','nltk','pynlpl','pymc','pgmpy','libpgm','deeppy','nolearn','hebel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 https://wiki.python.org/moin/PythonForArtificialIntelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toolkit_list7 = ['aima','pydatalog','simpleai','easyai','graphlab','featureforge','orange','pybrain','pyml','mlpy','milk','scikit-learn','shogun','mdp-toolkit','libsvm','weka','monte','som','yalign','nltk','gensim','quepy','neurolab','ffnet','fann','pyann','pyrenn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine and edit lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toolkits = list(set(toolkit_list1 + toolkit_list2 + toolkit_list3 + toolkit_list4 + toolkit_list5 + toolkit_list6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sorted(toolkits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkits.remove('ctnk')\n",
    "toolkits.remove('microsoft cognitive toolkit')\n",
    "# ctnk is typo, microsoft cognitive toolkit is cntk\n",
    "\n",
    "toolkits.remove('digits')\n",
    "toolkits.append('nvidia digits')\n",
    "\n",
    "toolkits.remove('neon')\n",
    "toolkits.append('nervana neon')\n",
    "\n",
    "toolkits.remove('paddle')\n",
    "toolkits.append('paddlepaddle')\n",
    "\n",
    "toolkits.remove('neural designer')\n",
    "toolkits.remove('wolfram mathematica')\n",
    "#neural designer and wolfram mathematica are proprietary with no github repositories \n",
    "\n",
    "toolkits.remove('mxnet')\n",
    "#mxnet is already in list as apache mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../data/dlai_toolkits_final.txt\", \"w\") as f:\n",
    "    f.writelines(\" \".join(toolkits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toolkits.sort()\n",
    "print toolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../code/secrets/github-token.nogit\", \"rb\") as f:\n",
    "    token = f.read()\n",
    "\n",
    "headers = {'Authorization': 'token %s' % token}\n",
    "\n",
    "@sleeper(3)#github will temporarily block requests from a user that makes more than 30 requests within a 60 second period. To be safe, use a 3 second pause between requests to limit rate to 20 requests per minute.\n",
    "@retry(wait_exponential_multiplier=3000,wait_exponential_max=12000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 3 second wait period and doubling that period each time.\n",
    "def get_data_from_search_helper(query):\n",
    "    r = requests.get('https://api.github.com/search/repositories?q='+\\\n",
    "                             query, \n",
    "                     headers=headers)\n",
    "    return r\n",
    "\n",
    "def get_data_from_search(query):\n",
    "    \"\"\"Use github search to return stars, forks for top query result\"\"\"\n",
    "    \n",
    "    r = get_data_from_search_helper(query)\n",
    "    #r.raise_for_status()\n",
    "    try:\n",
    "        res = r.json()['items'][0]\n",
    "        return {'toolkit': query, 'full_name': res['full_name'],\n",
    "                'stars': int(res['stargazers_count']), 'forks': int(res['forks_count'])}\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [res for res in (get_data_from_search(q) for q in toolkits)\n",
    "        if res is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github = pd.DataFrame(data)[['toolkit', 'full_name', 'forks', 'stars']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#github search returned wrong repo for mxnet, nvidia digits, torch, paddlepaddle, and intel bigdl- so they are changed manually below\n",
    "r = requests.get('https://api.github.com/repos/apache/incubator-mxnet', headers=headers)\n",
    "res = r.json()\n",
    "github.loc[github['toolkit'] == 'apache mxnet', 'full_name'] = 'apache/incubator-mxnet'\n",
    "github.loc[github['toolkit'] == 'apache mxnet', 'forks'] = res['forks_count']\n",
    "github.loc[github['toolkit'] == 'apache mxnet', 'stars'] = res['stargazers_count']\n",
    "\n",
    "r = requests.get('https://api.github.com/repos/NVIDIA/DIGITS', headers=headers)\n",
    "res = r.json()\n",
    "github.loc[github['toolkit'] == 'nvidia digits', 'full_name'] = 'NVIDIA/DIGITS'\n",
    "github.loc[github['toolkit'] == 'nvidia digits', 'forks'] = res['forks_count']\n",
    "github.loc[github['toolkit'] == 'nvidia digits', 'stars'] = res['stargazers_count']\n",
    "\n",
    "r = requests.get('https://api.github.com/repos/torch/torch7', headers=headers)\n",
    "res = r.json()\n",
    "github.loc[github['toolkit'] == 'torch', 'full_name'] = 'torch/torch7'\n",
    "github.loc[github['toolkit'] == 'torch', 'forks'] = res['forks_count']\n",
    "github.loc[github['toolkit'] == 'torch', 'stars'] = res['stargazers_count']\n",
    "\n",
    "r = requests.get('https://api.github.com/repos/PaddlePaddle/Paddle', headers=headers)\n",
    "res = r.json()\n",
    "github.loc[github['toolkit'] == 'paddlepaddle', 'full_name'] = 'PaddlePaddle/Paddle'\n",
    "github.loc[github['toolkit'] == 'paddlepaddle', 'forks'] = res['forks_count']\n",
    "github.loc[github['toolkit'] == 'paddlepaddle', 'stars'] = res['stargazers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "github.sort_values(['stars'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github.to_csv(\"../data/DLAI_toolkits_results_github.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stackoverflow stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_list = [toolkit.replace(' ','-') for toolkit in toolkits]\n",
    "#tag_counts = gso.get_tag_counts(tag_list)\n",
    "\n",
    "#function for getting stack overflow tag counts\n",
    "@sleeper(3)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=60000,wait_exponential_max=240000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 1 minute wait period and doubling that period each time.\n",
    "def tag_counts_builder_helper(list_to_build,list_entry):\n",
    "    list_to_build += gso.get_tag_counts([list_entry])\n",
    "\n",
    "#build list of tag counts\n",
    "tag_counts = []\n",
    "\n",
    "for x in tag_list:\n",
    "    tag_counts_builder_helper(tag_counts,x)\n",
    "\n",
    "df_tags = pd.DataFrame(tag_counts)[['name', 'count']]\n",
    "df_tags.columns = ['toolkit', 'so_tag_counts']\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@sleeper(3)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=60000,wait_exponential_max=240000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 1 minute wait period and doubling that period each time.\n",
    "#function used in building a dict each of whose values is the body count of the corresponding key\n",
    "def body_counts_builder_helper(dict_to_build,key):\n",
    "    dict_to_build[key] = gso.get_body_count([key])\n",
    "\n",
    "#build dict of body counts\n",
    "body_counts = {}\n",
    "i = 0\n",
    "\n",
    "for x in tag_list:\n",
    "    i += 1\n",
    "    print i\n",
    "    body_counts_builder_helper(body_counts,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_questions = pd.DataFrame.from_dict(body_counts, orient='index')\n",
    "df_questions.reset_index(inplace=True)\n",
    "df_questions.columns = ['toolkit', 'so_question_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so = df_tags.merge(df_questions, on='toolkit', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "so['toolkit'] = so['toolkit'].apply(lambda x: str(x).replace('-',' '))\n",
    "so.sort_values(['so_tag_counts'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##chainer is over counted in questions because chainer is a common word, opting for using chainer framework instead\n",
    "so.loc[so['toolkit'] == 'chainer', 'so_question_count'] = gso.get_body_count('chainer framework')\n",
    "\n",
    "##likewise with pattern\n",
    "so.loc[so['toolkit'] == 'pattern', 'so_question_count'] = gso.get_body_count('pattern web mining')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so.to_csv(\"../data/DLAI_toolkits_results_stackoverflow.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### google search results stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../code/secrets/google_token.nogit\", \"rb\") as f:\n",
    "    my_api_key = f.read()\n",
    "    \n",
    "with open(\"../code/secrets/cse_token.nogit\", \"rb\") as f:\n",
    "    my_cse_id = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_term_modifier(search_term):\n",
    "    #replace space with +, indicating to search for both words:\n",
    "    search_term = search_term.replace(' ','+')\n",
    "    #Since pattern is word that appears commonly in deep learning and ai applications unrelated to the library called pattern, \n",
    "    #we need to make the search more specific. At the same time, we want to avoid eliminating relevant results. To do this,\n",
    "    #we include the term web mining which we expect will appear in most pages relevant to the pattern library. This is not a perfect fix, \n",
    "    #as we will still get results for those search terms having nothing to do with the pattern library.\n",
    "    #Note that the concern we have for pattern does not apply to other common words like shogun and sonnet since our custom search\n",
    "    #includes the restriction that the result contain either the term \"deep learning\", \"artificial intelligence\", or \"machine learning\".\n",
    "    if (search_term == 'pattern'):\n",
    "        search_term = 'pattern+web+mining'\n",
    "    return search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function for getting number of google search results\n",
    "def google_search_results_count(search_term, api_key, cse_id):\n",
    "    toolkit = search_term\n",
    "    search_term = search_term_modifier(search_term)\n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"deep+learning\"+OR+\"artifical+intelligence\"+OR+\"machine learning\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=y5&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    return {'toolkit': toolkit, 'search_results': int(res['totalResults'])}\n",
    "\n",
    "#function for getting growth rate of google search results\n",
    "def google_quarterly_growth_rate(search_term, api_key, cse_id):\n",
    "    toolkit = search_term\n",
    "    search_term = search_term_modifier(search_term)\n",
    "    ##get count for last 6 months--- dateRestrict=m6\n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"deep+learning\"+OR+\"artifical+intelligence\"+OR+\"machine learning\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=m6&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    six_months = int(res['totalResults'])\n",
    "    ##get count for last 3 months--- dateRestrict=m3    \n",
    "    r= requests.get('https://www.googleapis.com/customsearch/v1?q=\"deep+learning\"+OR+\"artifical+intelligence\"+OR+\"machine learning\"&alt=json&cx='+\n",
    "                    my_cse_id+'&c2coff=1&dateRestrict=m3&exactTerms='+search_term+'&rc=1&key='+my_api_key)\n",
    "    res = r.json()['queries']['request'][0]\n",
    "    current_quarter = int(res['totalResults'])\n",
    "    \n",
    "    last_quarter = six_months - current_quarter\n",
    "    if (last_quarter == 0):#for handling the divide by 0 case\n",
    "        growth_rate = float('NaN')\n",
    "    else:\n",
    "        growth_rate = (float(current_quarter)-float(last_quarter))/float(last_quarter)\n",
    "    return {'toolkit': toolkit, 'growth_rate': growth_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@sleeper(2)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=2000,wait_exponential_max=8000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 2 second period and doubling that period each time.\n",
    "#function used in building a list each of whose values is the google search results count\n",
    "def google_results_builder_helper(list_to_build,query):\n",
    "    res = google_search_results_count(query, my_api_key, my_cse_id)\n",
    "    if res is not None:\n",
    "        list_to_build.append(res)\n",
    "        \n",
    "results = []\n",
    "for q in toolkits:\n",
    "    google_results_builder_helper(results,q)\n",
    "    \n",
    "resultsDF = pd.DataFrame(results)[['toolkit', 'search_results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsDF.sort_values(['search_results'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultsDF.sort_values(['search_results'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@sleeper(8)#attempt to avoid throttling\n",
    "@retry(wait_exponential_multiplier=8000,wait_exponential_max=32000,stop_max_attempt_number=3)#in case request fails, retry up to 3 times, starting with a 2 second period and doubling that period each time.\n",
    "#function used in building a list each of whose values is the google quarterly growth rate\n",
    "def growth_rate_builder_helper(list_to_build,query):\n",
    "    res = google_quarterly_growth_rate(query, my_api_key, my_cse_id)\n",
    "    if res is not None:\n",
    "        list_to_build.append(res)\n",
    "\n",
    "#build list of growth rates\n",
    "growth_rate = []\n",
    "i = 0\n",
    "for q in toolkits:\n",
    "    print q\n",
    "    print i\n",
    "    i += 1\n",
    "    growth_rate_builder_helper(growth_rate,q)\n",
    "    \n",
    "growthDF =  pd.DataFrame(growth_rate)[['toolkit', 'growth_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "growthDF.sort_values(['growth_rate'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googleDF = growthDF.merge(resultsDF, on='toolkit', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "googleDF.to_csv(\"../data/DLAI_toolkits_results_google.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dltkDF = github.merge(so, on='toolkit', copy = False)\n",
    "dltkDF = dltkDF.merge(googleDF, on='toolkit', copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dltkDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dltkDF.to_csv(\"../output/deep_learning_and_ai_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
